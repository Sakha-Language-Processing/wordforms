{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "verbs_at_end",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMhvzxsN+a4BJKCUmLoTCdo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakha-Language-Processing/wordforms/blob/main/verbs_at_end.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpPPmsov6nsv"
      },
      "source": [
        "#Автоматизированный поиск глаголов как последних слов в предложении\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pIgGC7aZjAc"
      },
      "source": [
        "Из 541024 предложений выделены последние слова, которые с большой вероятностью являются глаголами. Получилось 20408 словоформ. Доля глаголов является высокой.\n",
        "\n",
        "Около 80% текста потеряно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmDDBcvNYsIB"
      },
      "source": [
        "## Загрузка и очистка "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b1OQBzT_w4_",
        "outputId": "22972c1f-49d9-47f4-9bfc-290e3b75e2e9"
      },
      "source": [
        "!wget https://github.com/Sakha-Language-Processing/parser-palament/raw/main/raw_data_parlamentsakha.zip\n",
        "!unzip raw_data_parlamentsakha.zip\n",
        "!wget https://github.com/Sakha-Language-Processing/parser-nvkonline/raw/main/raw_data_nvkonline.zip\n",
        "!unzip raw_data_nvkonline.zip\n",
        "!wget https://github.com/Sakha-Language-Processing/sakha-parser-keskil/raw/main/raw_data_keskil_fixed.zip\n",
        "!unzip raw_data_keskil_fixed.zip\n",
        "!wget https://github.com/Sakha-Language-Processing/parser-kyym/raw/main/kyym-df.zip\n",
        "!unzip kyym-df.zip\n",
        "!wget https://github.com/Sakha-Language-Processing/parser-ysia/raw/main/sakha_ysia_df.zip\n",
        "!unzip sakha_ysia_df.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-18 13:28:10--  https://github.com/Sakha-Language-Processing/parser-palament/raw/main/raw_data_parlamentsakha.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/Sakha-Language-Processing/parser-parlament/raw/main/raw_data_parlamentsakha.zip [following]\n",
            "--2021-04-18 13:28:10--  https://github.com/Sakha-Language-Processing/parser-parlament/raw/main/raw_data_parlamentsakha.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Sakha-Language-Processing/parser-parlament/main/raw_data_parlamentsakha.zip [following]\n",
            "--2021-04-18 13:28:10--  https://raw.githubusercontent.com/Sakha-Language-Processing/parser-parlament/main/raw_data_parlamentsakha.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3857643 (3.7M) [application/zip]\n",
            "Saving to: ‘raw_data_parlamentsakha.zip.1’\n",
            "\n",
            "raw_data_parlaments 100%[===================>]   3.68M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-04-18 13:28:10 (29.7 MB/s) - ‘raw_data_parlamentsakha.zip.1’ saved [3857643/3857643]\n",
            "\n",
            "Archive:  raw_data_parlamentsakha.zip\n",
            "replace raw_data_parlamentsakha.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: --2021-04-18 13:28:16--  https://github.com/Sakha-Language-Processing/parser-nvkonline/raw/main/raw_data_nvkonline.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Sakha-Language-Processing/parser-nvkonline/main/raw_data_nvkonline.zip [following]\n",
            "--2021-04-18 13:28:16--  https://raw.githubusercontent.com/Sakha-Language-Processing/parser-nvkonline/main/raw_data_nvkonline.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1965991 (1.9M) [application/zip]\n",
            "Saving to: ‘raw_data_nvkonline.zip.1’\n",
            "\n",
            "raw_data_nvkonline. 100%[===================>]   1.87M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-04-18 13:28:16 (28.9 MB/s) - ‘raw_data_nvkonline.zip.1’ saved [1965991/1965991]\n",
            "\n",
            "Archive:  raw_data_nvkonline.zip\n",
            "replace raw_data_nvkonline.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: --2021-04-18 13:28:17--  https://github.com/Sakha-Language-Processing/sakha-parser-keskil/raw/main/raw_data_keskil_fixed.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Sakha-Language-Processing/sakha-parser-keskil/main/raw_data_keskil_fixed.zip [following]\n",
            "--2021-04-18 13:28:17--  https://raw.githubusercontent.com/Sakha-Language-Processing/sakha-parser-keskil/main/raw_data_keskil_fixed.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 430880 (421K) [application/zip]\n",
            "Saving to: ‘raw_data_keskil_fixed.zip.1’\n",
            "\n",
            "raw_data_keskil_fix 100%[===================>] 420.78K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-04-18 13:28:17 (13.6 MB/s) - ‘raw_data_keskil_fixed.zip.1’ saved [430880/430880]\n",
            "\n",
            "Archive:  raw_data_keskil_fixed.zip\n",
            "replace raw_data_keskil.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "--2021-04-18 13:28:33--  https://github.com/Sakha-Language-Processing/parser-kyym/raw/main/kyym-df.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Sakha-Language-Processing/parser-kyym/main/kyym-df.zip [following]\n",
            "--2021-04-18 13:28:33--  https://raw.githubusercontent.com/Sakha-Language-Processing/parser-kyym/main/kyym-df.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8931048 (8.5M) [application/zip]\n",
            "Saving to: ‘kyym-df.zip.1’\n",
            "\n",
            "kyym-df.zip.1       100%[===================>]   8.52M  46.1MB/s    in 0.2s    \n",
            "\n",
            "2021-04-18 13:28:33 (46.1 MB/s) - ‘kyym-df.zip.1’ saved [8931048/8931048]\n",
            "\n",
            "Archive:  kyym-df.zip\n",
            "replace kyym-novosti.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: kyym-novosti.csv        \n",
            "--2021-04-18 13:28:46--  https://github.com/Sakha-Language-Processing/parser-ysia/raw/main/sakha_ysia_df.zip\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Sakha-Language-Processing/parser-ysia/main/sakha_ysia_df.zip [following]\n",
            "--2021-04-18 13:28:46--  https://raw.githubusercontent.com/Sakha-Language-Processing/parser-ysia/main/sakha_ysia_df.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8413663 (8.0M) [application/zip]\n",
            "Saving to: ‘sakha_ysia_df.zip.1’\n",
            "\n",
            "sakha_ysia_df.zip.1 100%[===================>]   8.02M  47.2MB/s    in 0.2s    \n",
            "\n",
            "2021-04-18 13:28:47 (47.2 MB/s) - ‘sakha_ysia_df.zip.1’ saved [8413663/8413663]\n",
            "\n",
            "Archive:  sakha_ysia_df.zip\n",
            "replace sakha_ysia.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0UeV99aEmLI"
      },
      "source": [
        "# Чтение файлов\n",
        "# Можно добавить свои список файлов \n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "articles = []\n",
        "for file in ['raw_data_nvkonline.json',\n",
        "             'raw_data_parlamentsakha.json',\n",
        "             'raw_data_keskil.json']:\n",
        "    file_ = open(file) \n",
        "    articles  += json.load(file_)\n",
        "    file_.close()\n",
        "\n",
        "all_files_text = []\n",
        "for article in articles:\n",
        "    all_files_text += article['raw_text'] \n",
        "    if 'title' in article:\n",
        "        all_files_text += [article['title']]\n",
        "\n",
        "aytal_text = ' '\n",
        "for file in ['kyym-novosti.csv',\n",
        "             'sakha_ysia.csv']:\n",
        "    file_ = pd.read_csv(file) \n",
        "    aytal_text  += ' '.join(list(file_['title'].fillna(' ')+file_['all_text'].fillna(' ')))\n",
        "\n",
        "all_files_text = ' '.join(all_files_text) + ' '.join(aytal_text)\n",
        "del aytal_text"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMxmnIDkWgoV"
      },
      "source": [
        "textslist = all_files_text.split('  ')\n",
        "all_files_text = ' '.join([item for item in textslist if len(item)>1])\n",
        "all_files_text += ' '.join([item for item in textslist if len(item)==1])\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRmRVLIARCwv"
      },
      "source": [
        "#  в некоторых статьях буквы стоят отдельно\n",
        "\n",
        "tezt = all_files_text.replace(' ','')\n",
        "# print(n)\n",
        "# print(all_files_text[n-100:n+300])\n",
        "\n",
        "\n",
        "# print('Имеется', len([item for item in all_files_text.split(' ')  if len(item)>1]), 'слов.')\n",
        "# x1,x2 = all_files_text[:100000].count('ы'),all_files_text.count(' ы ')\n",
        "# x_slov = len(all_files_text[:100000].split(' '))\n",
        "# print('Потеряно примерно', int(x_slov/x1*x2), 'слов.')\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyqdHMQWWLDB",
        "outputId": "3222ff7c-3419-422f-85ee-0665f1af2035"
      },
      "source": [
        "len(tezt.split(' '))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "Bms_LkL5SkVo",
        "outputId": "26b696a6-6c54-41ea-ca58-c9f2c3868777"
      },
      "source": [
        "n = 14245000\n",
        "all_files_text[n-300:n+300]\n"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'т .   К и н и   м а н н а   б и и р   д о й д у л а а х т а р ы н   к ы т а р ы   к ө р с ү б ү т .   К о н с у л   т о й о н   Т а р а с е н к о н ы   У з б е к и с т а ҥ ҥ а   ы а л д ь ы т т а т а   ы ҥ ы р б ы т .   Х а а р т ы с к а н ы   М а р и я   В а с и л ь е в а ,   С И А   т ү һ э р б и т .     Э Л И М И Э Н     Э Р Э Й   Д У У ,   Э Р Э Л   Д У У ?   А ҕ ы й а х   с ы л л а а ҕ ы т а   А н а а б ы р г а   ү л э л и и   с ы л д ь а н ,   б и и р   у л а х а н   у ч а а с т а к к а   ү л э б и т   т ү б ү г э р   с ы л д ь а н   б а р а м м ы т ,   б э й э б и т   к ы р а к ы й   у '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qYfkBHxKaX3"
      },
      "source": [
        "import re\n",
        "all_files_text = re.sub(re.compile(r'https?\\S+'), ' ', all_files_text)\n",
        "all_files_text = re.sub(re.compile(r'<.*?>'), ' ', all_files_text)\n",
        "all_files_text = re.sub(re.compile(r'@\\S+'), ' ', all_files_text)\n",
        "all_files_text = re.sub(re.compile(r'[\\//:@&\\-\\'\\`\\\"\\_\\n\\#]'), ' ', all_files_text)\n",
        "all_files_text = re.sub(re.compile(r'\\s*[A-Za-z\\d.]+\\b'), ' ' , all_files_text)\n",
        "all_files_text = all_files_text.lower()\n",
        "similar_characters = {'h':'һ','c':'с','x':'х','y':'у',\n",
        "                      'e':'е', 'T':'Т', 'o':'о', 'p':'р', \n",
        "                      'a':'а', 'k':'к', 'b':'в', 'm':'м', \n",
        "                      '5':'ҕ', '8':'ө',\n",
        "                      # исправления\n",
        "                      'ааа':'аа','ттт':'тт','ыыы':'ыы',\n",
        "                      'ууу':'уу','   ':' ','  ':' ',\n",
        "                      'иии':'ии','эээ':'ээ','үүү':'үү',\n",
        "                      'өөө':'өө','өү':'үө','ччч':'чч',\n",
        "                      'ннн':'нн','ддд':'дд','ллл':'лл',    \n",
        "                      'ммм':'мм','ххх':'хх',    \n",
        "                      }                      \n",
        "for loaned, ersatz in similar_characters.items():\n",
        "    all_files_text = all_files_text.replace(loaned, ersatz)  \n",
        "\n",
        "all_files_text = re.sub(re.compile(r'[.!?]'), ' ', all_files_text)\n",
        "all_files_text = all_files_text.replace('      ',' ')\n",
        "all_files_text = all_files_text.replace('   ',' ')\n",
        "all_files_text = all_files_text.replace('  ',' ')\n",
        "all_files_text = all_files_text.replace('  ',' ')\n",
        "all_files_text.count('  ')\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4srjz7ztOkdG"
      },
      "source": [
        "ddd = [wordform.strip() for wordform in all_files_text.split(' ') if len(wordform)>1]\n",
        "ddd = sorted(ddd)\n",
        "\n",
        "words = []\n",
        "counts = []\n",
        "counter = 0\n",
        "for item1,item2 in zip(ddd[:-1],ddd[1:]):\n",
        "    counter += 1\n",
        "    if item1!=item2:\n",
        "       words.append(item1)\n",
        "       counts.append(counter)\n",
        "       counter = 0\n",
        "df = pd.DataFrame({'wordform':words,'occurence':counts})          "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVbFV59NYvl7"
      },
      "source": [
        "## Выделение последних слов предложений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K7cA_piXIAm"
      },
      "source": [
        "\n",
        "all_files_text = all_files_text.replace('?', '.')\n",
        "all_files_text = all_files_text.replace('!', '.')\n",
        "regex = re.compile('[^а-яА-ЯһөҕүҥҺӨҔҮҤ. ]')\n",
        "all_files_text = re.sub(regex, ' ', all_files_text)\n",
        "\n",
        "sentence_tails = [item[-min(len(item),30):] for item in all_files_text.split('.')]\n",
        "\n",
        "verbs = []\n",
        "for item in sentence_tails:\n",
        "    verb = item.split(' ')[-1]\n",
        "    if len(verb)<1:\n",
        "         for i in range(30):\n",
        "            item = item.replace(' .','.')\n",
        "            verb = item.split(' ')[-1]\n",
        "    verbs.append(verb) \n",
        "verbs = sorted(list(set(verbs)))\n",
        "df = pd.DataFrame(verbs)\n",
        "df.columns = ['verb']\n",
        "df.to_csv('verbs.csv',index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9R-t8CuUBMZ",
        "outputId": "2e1b9b81-b0ce-49eb-ce36-f496b1c1ddc5"
      },
      "source": [
        "len(sentence_tails)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "541024"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1E37oqAYhdZ"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}